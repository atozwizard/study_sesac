[장표 1: 서론] 대규모 언어 모델의 긴 문맥 처리 한계와 최적화 전략
안녕하세요. AI/LLM 애플리케이션 엔지니어 지원자 [이름]입니다. 오늘 저는 거대해지는 컨텍스트 윈도우의 화려한 수치 뒤에 숨겨진 '문맥 부패' 현상을 진단하고, 이를 해결하기 위한 다층적 엔지니어링 전략을 발표하겠습니다.

[장표 2: 현상 및 문제점] 문맥 확장(Context Extension)의 허상과 '문맥 부패'
최근 모델들은 100만 토큰 이상의 입력을 지원하지만, 실제로는 입력이 길어질수록 모델의 추론 능력과 이해도가 하락하는 **'문맥 부패(Context Rot)'**를 겪습니다. 이는 단순히 많은 데이터를 넣는 것이 성능 향상을 보장하지 않으며, 오히려 노이즈가 늘어날 때 모델의 신뢰도가 급격히 떨어진다는 것을 의미합니다.

[장표 3: 현상 분석] Lost in the Middle (중간 소실 현상)
이 부패의 구체적인 양상은 '잃어버린 중간' 현상으로 나타납니다. 모델은 입력의 시작 부분인 **초두 효과(Primacy Bias)**와 끝부분인 **최신 효과(Recency Bias)**에 과도한 어텐션을 할당하는 고유한 U자형 편향을 가집니다.  LLM이 입력 정보의 시작과 끝부분은 비교적 잘 활용하지만, 중간에 위치한 정보는 간과하거나 무시하여 성능이 급격히 저하되는 경향을 의미합니다.  때로는 정보를 전혀 주지 않은 상태보다 낮은 정확도를 보이기도 합니다. 이 문제는 특히 검색 증강 생성(Retrieval-Augmented Generation, RAG) 시스템의 부상으로 인해 핵심적인 관심사로 떠올랐습니다. RAG 시스템의 신뢰성은 검색된 정보의 순서와 무관하게 모델이 이를 충실히 활용하는 능력에 달려 있기 때문입니다

[장표 5: 세부 전략 ①] 데이터 전처리 및 청킹(Chunking) 최적화
RAG(Retrieval-Augmented Generation) 시스템에서
청킹은 단순한 전처리가 아니라 검색과 생성 품질을 좌우하는 핵심 설계 요소예요. 여러 연구에서도 동일한 문서·임베딩 모델 환경에서 청킹 전략만 달라져도 Recall이 최대 9%p까지 변한다는 결과가 보고되었죠. 즉, 문서를 어떤 단위로 나누느냐는 RAG 성능의 토대를 결정하는 문제라고 볼 수 있어요.


문장 단위 청킹이 무조건 정밀도가 높은가?"
    ◦너무 작은 청크는 오히려 nDCG(검색 품질 지표)가 하락할 수 있음을 경고합니다. 따라서 문장 단위의 장점은 '특정 사실 검색'에 국한되며, 일반적인 추론에는 부적합할 수 있다

• 문장 단위 청킹: 
아주 좁은 범위의 사실관계를 찾는 데 유리하여 **검색 정밀도(Precision)**를 높일 수 있습니다. 그러나 문맥이 지나치게 파편화되어 정보의 전후 논리 관계를 상실하는 문맥 단절(Fragmentation) 리스크가 큽니다. 구현이 간단하고 대규모 처리에 유리하지만 의미적 단절이 발생할 수 있습니다. 이를 보완하기 위해 10~25%의 오버랩(Overlap) 설정이 필수적


• 단락 단위 청킹: 
문장 간 유사도를 분석해 의미적 일관성을 유지하며 청크를 나눕니다.
작성자의 의도가 담긴 논리적 단위를 유지하므로 구조 기반(Structure-aware) 이해도가 높습니다. 모델이 청크 하나만으로도 충분한 배경 지식을 얻을 수 있어 답변의 안정성은 우수하지만, 단락 내 무관한 내용이 섞일 경우 핵심 정보가 흐릿해지는 정보 희석(Information Dilution) 문제가 발생할 수 있습니다. 


• 최적화 제안: 
따라서 단락 단위를 기본으로 하되, 문맥 단절을 보완하기 위해 전체 길이의 **10~25%를 오버랩(Overlap)**으로 설정하는 것이 가장 효율적입니다.

결론적으로, 모든 문서에 통용되는 단일 규칙은 없으며 문서의 성격과 정보 밀도에 따라 전략적 실험을 통해 최적의 파라미터를 찾는 과정이 중요합니다.




[장표 6: 세부 전략 ②] 검색 및 리랭킹(Reranking) 기법

청킹된 문서들 중 관련성이 높은 문서를 찾아낸 뒤에는 모델의 편향을 역이용해야 합니다. 리랭킹은 가장 관련성이 높은 문서를 모델이 잘 보는 '시작'이나 '끝' 위치에 의도적으로 재배치하여 성능 급락을 방지하는 외적 조작 기법입니다.



[장표 8: 세부 전략 ③] 추론 단계 어텐션 보정 (Found-in-the-Middle)

더 나아가 추론 시점에 모델 내부 어텐션 구조를 직접 보정하는 Found-in-the-Middle 기술이 있습니다. **더미 문서(Dummy Document)**를 통해 위치별로 발생하는 '가짜 어텐션 편향'을 먼저 측정한 뒤, 실제 질의 시의 어텐션 값에서 이 편향치를 차감(Subtraction)합니다. 이를 통해 추가 훈련 없이도 RAG 정확도를 최대 15%p 향상시킬 수 있습니다.

(Found-in-the-Middle):  추론 시점에 모델의 주의 메커니즘에 직접 개입하여 위치적 편향을 제거하는  '사후 보정(post-hoc calibration)'  방식입니다. 기존 모델을 수정 없이 즉시 적용할 수 있는 유연한 접근법입니다



[장표 9: 세부 전략 ④] 모델 훈련 최적화 (PAM QA)
주의 보정과는 다른 접근법으로, 모델이 정보의 위치에 구애받지 않도록 근본적인 체질을 개선하는 특수 훈련 전략이 있습니다

가장 근본적인 해결책은 모델의 체질을 바꾸는 PAM QA 훈련입니다. 모델에게 **"질문 복기 → 근거 인덱스 예측 → 답변 요약"**의 3단계를 수행하도록 미세 조정(SFT)합니다. 이렇게 학습된 모델은 답변 전 스스로 '메모'를 남기는 효과를 얻어, 정보의 위치와 상관없이 필요한 곳에 강력한 **어텐션 피크(Peak)**를 형성하여 99%에 달하는 검색 정확도를 유지합니다.


질문 반복 (Question Repetition):  모델은 먼저 주어진 질문을 그대로 반복하여 생성합니다. 이는 모델이 후속 처리 과정을 핵심 과제에 고정시켜 불필요한 정보에 의해 주의가 분산되는 것을 방지합니다.

인덱스 예측 (Index Prediction):  다음으로, 모델은 제공된 여러 문서 중에서 정답의 근거가 되는 문서의 **색인(index)**을 예측합니다. 이것이 위치적 편향을 깨는 결정적인 단계입니다. 모델이 관련 문서의 '색인'을 명시적으로 식별하도록 강제함으로써, 편향된 끝점을 가진 연속적인 시퀀스가 아닌, 검색 가능하고 주소 지정이 가능한 공간으로 컨텍스트를 처리하도록 학습합니다.

답변 요약 (Answer Summarization):  마지막으로, 앞선 단계에서 파악한 질문(1단계)과 예측한 근거 문서(2단계)를 종합하여 최종 답변을 생성합니다. 이 마지막 단계는 사전 식별된 관련 정보에만 근거하여 답변을 종합하도록 훈련시켜 생성된 답변의 충실도(faithfulness)를 높입니다.

이 훈련 방식을 거친 모델은 기존 모델과 뚜렷한 차이를 보입니다. PAM QA 훈련을 받은 모델의 주의 가중치는 컨텍스트 전반에 걸쳐 훨씬 더 균등하게 분포하며
, U자형 편향이 거의 사라집니다. 그 결과, 관련 문서가 컨텍스트의 어느 위치에 있든  합성 과제(Synthetic Task)에서  거의 완벽에 가까운(99%) 안정적인 성능을 유지합니다. 이는 다른 모델들이 정답의 위치에 따라 성능이 급격히 저하되는 것과 매우 대조적인 결과입니다.

[장표 15: 핵심 요약] Executive Summary
결론입니다. AI의 성능은 얼마나 많은 정보를 보여주느냐가 아니라, **모델이 정보를 어떻게 보게 만드느냐(Attention Control)**에 달려 있습니다. 문맥의 '양'보다 '밀도와 질'을 관리하는 단계별 컨텍스트 엔지니어링이 차세대 RAG 시스템의 핵심 경쟁력이 될 것입니다.

--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
