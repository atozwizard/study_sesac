. PEFT(Parameter Efficient Fine-Tuning)이란?
PEFT는 적은 매개변수 학습만으로 빠른 시간에 새로운 문제를 효과적으로 해결하는 미세조정 방법을 지칭한다. 수천억 개 이상의 매개변수를 가진 초거대언어모델(LLM)도 사실상 적은 파라미터만 조절하여 유사한 성능을 낼 수 있다는 연구결과가 있었는데, 이를 기반으로 PEFT 방법론 연구가 활발히 진행되고 있다



양자화(Quantization)
양자화 역시 PEFT의 한 방법론으로, 언어 모델의 매개변수를 실수형에서 정수형으로 바꾸어 비트 수를 줄이는 과정을 말한다. 예컨대 32비트 부동 소수점 형태의 매개변수를 8비트 정수로 변환하는 것과 같이 비트 수를 감소시켜서 모델 사이즈를 줄이는 방식이다.

양자화된 언어 모델은 크기가 줄어들며, 계산의 효율성이 향상된다. 

비트 수를 N배로 줄이면 곱셈의 복잡도는 NxN로 감소하게 되며, 이에 따라 float32를 사용하는 대신 int8을 사용하면 모델의 크기가 1/4로 줄어들고, 추론(inference) 속도와 메모리 사용량도 두 배에서 네 배까지 효율적으로 작동하게 된다.





LoRA(Low Rank Adoption)
LoRA(낮은 순위 적응)는 PEFT 방법론 중 하나로, 대부분의 매개변수 가중치는 원래대로 유지하되 일부만 미세조정하는 방식을 사용한다. 이렇게 함으로써 훈련 비용과 컴퓨팅 리소스를 절약하면서도 특정 작업의 성능을 향상시킬 수 있다.

LoRA 작동 원리를 간략히 설명하면, 행렬의 차원을 'r'만큼 축소 후 원래 크기로 복원하는 것과, 은닉층 'h'에 특정 값을 추가하여 출력을 조절하는 것이 핵심이다. 이를 통해 원하는 출력값을 얻을 수 있게 된다.