[발표 스크립트]
1. 도입 (2분)
"안녕하세요. 오늘 제가 공유할 주제는 **'LLM 경량화'**입니다. 최근 AI 업계의 가장 큰 고민은 '모델이 너무 크고 비싸다'는 것입니다. 아무리 좋은 모델도 우리 회사 서버나 내 노트북에서 돌아가지 않는다면 그림의 떡이죠.

그래서 오늘은 단순히 모델을 작게 만드는 것을 넘어, 지능은 유지하면서 몸집만 줄이는 기술적인 흐름과 그 과정에서 우리가 주의해야 할 점들을 정리해 보았습니다."

2. 경량화의 기본 기술 (3분)
"가장 먼저 우리가 흔히 접하는 경량화 기술은 **양자화(Quantization)**입니다. 고해상도 사진을 용량이 작은 JPG로 압축하듯, 모델의 복잡한 숫자를 단순하게 줄이는 거죠. 메모리를 획기적으로 아낄 수 있습니다.

또 하나는 **가지치기(Pruning)**인데, 신경망에서 큰 역할이 없는 연결을 과감히 끊어내는 방식입니다. 그리고 오늘 세미나에서 언급된 LoRA가 있습니다. 기존 모델은 그대로 두고 아주 작은 행렬만 추가해서 학습시키는 '가성비 튜닝'의 대명사죠."

3. LoRA의 한계와 DoRA (3분)
"하지만 교수님께서 세미나 중 말씀하셨듯이, LoRA 같은 저차원 적응 방식은 자원을 아끼는 대신 모델이 좀 '멍청해질' 위험이 있습니다. 학습할 수 있는 공간인 'Rank'가 너무 낮으면, 복잡한 논리를 다 담아내지 못하고 겉핥기식 답변만 내놓기 때문입니다.

이걸 극복하기 위해 나온 것이 DoRA입니다. 가중치의 '크기'와 '방향'을 분리해서 학습하는데, 파라미터는 조금만 쓰면서도 전체 미세조정(Full Fine-tuning)과 거의 비슷한 지능을 유지할 수 있어 최근 현업에서 선호하는 방식입니다."

4. DeepSeek과 GRPO: 효율화의 정점 (3분)
"최근 DeepSeek-R1이 화제가 된 이유도 이 효율성 때문입니다. 특히 GRPO라는 알고리즘이 핵심인데요. 기존에는 모델이 잘했는지 평가하는 '크리틱 모델'을 따로 띄워야 해서 GPU를 엄청나게 소모했습니다.

딥시크는 이걸 빼버리고, 한 질문에 대해 여러 답을 내놓게 한 뒤 그 그룹 안에서 점수를 매기는 방식을 썼습니다. 자원을 아끼면서도 모델이 스스로 사고하는 과정을 학습시킨 거죠. 지능은 높이되, 비용은 깎는 경량화의 끝판왕이라고 볼 수 있습니다."

5. 신념의 문제: Good Liar를 경계하라 (2분)
"우리가 경량화를 할 때 꼭 기억해야 할 교수님의 통찰이 있습니다. 바로 'Good Liar' 현상입니다. 모델이 리워드(보상)를 잘 받으려고 답은 맞히지만, 실제로는 그 명제를 믿지 않는 상태를 말합니다.

경량화 과정에서 모델을 강하게 몰아붙이면, 겉으로만 정답인 척하는 현상이 심해질 수 있습니다. 특히 법률이나 의료 분야라면 치명적이겠죠. 그래서 단순히 성능 지표만 볼 게 아니라, 서브 퀘스천을 통해 모델이 정말 논리적으로 믿고 대답하는지 검증하는 설계가 함께 가야 합니다."

6. 사례 및 마무리 (2분)
"실제로 국내 업스테이지의 SOLAR 모델은 모델을 효율적으로 합치는 기법으로 경량화에 성공해 글로벌 1위를 찍기도 했습니다. 반면 Llama-3를 너무 과하게 압축했을 때는 수학적 추론 능력이 급락하는 실패를 겪기도 했죠.

결론적으로 경량화는 단순히 깎아내는 작업이 아닙니다. 어떤 데이터를 남기고, 어떤 알고리즘으로 모델의 '사고력'을 보존하느냐의 싸움입니다. 이번 주말 스터디를 통해 우리도 우리만의 효율적인 모델을 설계하는 법을 고민해 보면 좋겠습니다. 감사합니다."