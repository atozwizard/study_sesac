예상 질문들을 15개 정도 뽑아보자. 청자들은 대체로 업계 새내기 초보자 수준인 나보다 조금 더 깊게 알고 있는 사람들이야. 주로 깊은 개념이나 동작 메커니즘에 대해 질문 할 가능성이 높고, 교수님의 good liar논문과, 경량화의 대표사례들, 향후 개발시 고려해야할 방안에 대해 질문할 가능성이 있어

스터디원들이 본인보다 조금 더 깊은 지식을 가지고 있다면, 단순한 정의보다는 **'왜(Why)'**와 **'어떻게(How)'**에 집중된 날카로운 질문이 나올 가능성이 높습니다. 특히 교수님의 연구 철학과 최신 기술의 충돌 지점을 파고들 것입니다.

예상 질문 15가지를 카테고리별로 정리해 드릴 테니, 답변 방향을 미리 시뮬레이션해 보세요.

1. 경량화 및 LoRA/DoRA 관련 질문
Q1. LoRA에서 Rank(r) 값을 무조건 높이면 '멍청함'이 해결되나요? 아니면 다른 부작용이 있나요?

답변 팁: Rank가 높으면 표현력은 좋아지지만, 학습 파라미터가 늘어나 메모리 이점이 사라지고 오버피팅(과적합) 위험이 커진다고 답하세요.

Q2. DoRA가 LoRA보다 성능이 좋다면, 현업에서 LoRA를 쓸 이유가 전혀 없나요?

답변 팁: DoRA는 연산 복잡도가 조금 더 높습니다. 아주 가벼운 튜닝이 목적이라면 여전히 LoRA가 경제적일 수 있습니다.

Q3. 양자화(Quantization)를 하면 모델의 추론 속도만 빨라지나요, 아니면 학습 속도도 빨라지나요?

답변 팁: 주로 추론 속도와 메모리 절약에 집중되지만, QLoRA 같은 기법을 쓰면 학습 시 필요한 메모리가 줄어들어 더 큰 모델을 학습할 수 있게 해준다고 설명하세요.

Q4. 가지치기(Pruning)를 한 모델은 하드웨어 가속기(GPU)에서 실제로 속도 향상을 체감하기 어렵다는데 왜 그런가요?

답변 팁: 비정형 가지치기는 행렬 연산을 불규칙하게 만들어 GPU의 병렬 연산 효율을 떨어뜨릴 수 있기 때문입니다. 그래서 구조적 가지치기(Structured Pruning)가 중요합니다.

2. DeepSeek 및 GRPO 관련 질문
Q5. GRPO에서 크리틱(Critic) 모델을 없애면 보상의 기준을 어떻게 잡나요?

답변 팁: 생성된 여러 답변의 평균 점수를 기준으로, 상대적으로 잘한 답변에 플러스, 못한 답변에 마이너스를 주는 '그룹 상대 평가' 방식을 쓴다고 설명하세요.

Q6. DeepSeek-R1처럼 사고 과정(CoT)을 증류(Distillation)하면, 작은 모델이 논리만 배우나요 아니면 나쁜 습관도 배우나요?

답변 팁: 큰 모델이 가진 편향성이나 '할루시네이션(환각)' 패턴까지 그대로 학습될 위험이 있습니다. 그래서 데이터 정제(Filtering)가 필수적입니다.

Q7. DPO와 GRPO의 가장 큰 차이점은 무엇인가요?

답변 팁: DPO는 정해진 데이터셋 안에서 선호를 배우는 '오프라인' 방식이고, GRPO는 모델이 직접 답을 내보며 실시간 보상을 받는 '온라인' 강화학습 방식입니다.

3. 김수경 교수님의 'Good Liar' 및 신념 관련 질문
Q8. 모델이 '믿지 않는 것'을 억지로 학습시키면 다른 문제를 못 풀게 된다고 하셨는데, 그 이유가 무엇일까요?

답변 팁: 모델의 내부 지식 간의 정합성이 깨지면서 전체적인 파라미터 분포가 왜곡되기 때문입니다(Catastrophic Interference).

Q9. '진실된 대답'과 '윤리적인 가이드라인에 따른 대답'이 충돌할 때는 어떻게 설계해야 하나요?

답변 팁: 매우 어려운 문제지만, 팩트(Fact)는 진실되게, 표현 방식(Tone)은 윤리적으로 분리하여 튜닝하는 기법들이 연구되고 있다고 답하세요.

Q10. 변호사 AI에서 '골든 앤서'를 도출할 때, 경량화 모델을 써도 신뢰성을 확보할 수 있을까요?

답변 팁: 경량 모델은 복잡한 법리 해석에서 누락이 생길 수 있으므로, RAG(검색 증강 생성) 기술을 결합하여 외부 데이터를 참조하게 함으로써 보완해야 합니다.

4. 실전 사례 및 향후 개발 고려 사항
Q11. 업스테이지의 SOLAR가 쓴 DUS 기법은 단순히 모델을 쌓는 것인데, 왜 지능이 높아지는 건가요?

답변 팁: 서로 다른 레이어의 지식을 겹쳐서 학습시킴으로써, 모델의 깊이(Depth)를 늘리면서도 기존 학습 지식을 효율적으로 재활용하기 때문입니다.

Q12. Llama-3 과압축 실패 사례에서 '지능 회복 학습'은 구체적으로 어떤 데이터를 써야 하나요?

답변 팁: 압축 과정에서 손실되기 쉬운 수학, 코드, 논리 추론 등 '고밀도 지식'이 담긴 데이터를 소량으로 다시 학습시켜야 합니다.

Q13. 앞으로 개발자로서 모델을 고를 때 파라미터 수 외에 무엇을 가장 먼저 봐야 할까요?

답변 팁: 해당 모델이 어떤 '포스트 트레이닝' 과정을 거쳤는지(DPO인지, RLHF인지)와 우리가 쓸 하드웨어(VRAM)에서 양자화 효율이 얼마나 나오는지 봐야 합니다.

Q14. 리워드 해킹을 방지하기 위한 프롬프트 기법 외에, 데이터 측면에서의 해결책은?

답변 팁: 정답만 있는 데이터가 아니라, '오답인 이유'가 설명된 데이터를 함께 학습시켜 모델의 추론 과정을 교정해야 합니다.

Q15. SLLM(소형언어모델) 시장에서 오픈소스 모델을 그대로 쓰는 것과 자체 경량화 모델을 만드는 것 중 무엇이 경제적일까요?

답변 팁: 도메인 특화 지식이 중요하다면 자체 경량화(LoRA 등)가 필수적이고, 일반적인 대화라면 잘 만들어진 오픈소스를 그대로 쓰는 게 낫습니다.

💡 발표자를 위한 조언: 스터디원들이 "그건 구체적으로 어떻게 구현하나요?"라고 물었을 때, **"제가 오늘 세미나에서 이해한 바로는 ~인데, 이 부분은 우리 스터디에서 같이 코드를 뜯어보며 확인해봐도 좋을 것 같아요."**라고 넘기면 아주 겸손하면서도 스터디의 본질에 맞는 훌륭한 대처가 됩니다.

혹시 이 질문들 중 답변이 더 막막한 부분이 있으신가요? 하나 골라주시면 상세 답변 시나리오를 짜드릴게요.

답변을 더 자세히, 그러나 쉽게 풀어서 해야할 필요가 있어., 로라, 도라는 무엇의 줄임말인지도 명시해.

가장 날카로운 질문이 들어올 수 있는 5가지 항목을 선정해서, 초보자의 언어로 풀었지만 핵심 메커니즘은 놓치지 않는 상세 답변 시나리오를 짜드리겠습니다.

1. LoRA와 DoRA의 차이 및 정의
Q: 로라와 도라가 정확히 뭐가 다른가요? 이름은 무슨 뜻이죠?

용어 정리:

LoRA (Low-Rank Adaptation): '저차원 적응'이라는 뜻입니다.

DoRA (Weight-Decomposed Low-Rank Adaptation): '가중치 분해 저차원 적응'이라는 뜻입니다.

쉬운 설명: "LoRA는 원래 모델 옆에 작은 **'메모장(포스트잇)'**을 하나 붙여서 거기다만 새로운 지식을 적는 방식이에요. 그런데 이 메모장이 너무 작다 보니, 원래 모델이 가진 지식과 새 지식이 엉키거나 표현이 단조로워지는 문제가 생깁니다. 반면 DoRA는 그 가중치를 **'크기(얼마나 강하게)'**와 **'방향(어디로)'**이라는 두 가지 성분으로 쪼개서 학습해요. 우리가 길을 찾을 때 '북쪽으로(방향) 100미터(크기)'라고 하는 것과 같죠. 이렇게 나눠서 학습하면 LoRA보다 훨씬 정교하게 지능을 유지할 수 있어서, 적은 자원으로도 전체를 다 학습시킨 것과 비슷한 성능이 나옵니다."

2. DeepSeek의 GRPO 메커니즘
Q: GRPO가 크리틱 모델 없이 어떻게 학습이 가능한가요?

쉬운 설명: "원래 강화학습은 **'학생(모델)'**이 답을 쓰면, 옆에서 **'선생님(크리틱 모델)'**이 실시간으로 '이건 80점이야, 이건 20점이야'라고 점수를 매겨줘야 합니다. 근데 이 선생님 모델이 학생만큼 덩치가 커서 GPU를 엄청나게 잡아먹거든요. GRPO는 이 선생님을 없애버렸어요. 대신 학생에게 한 번에 5~10개의 답안지를 쓰게 합니다. 그리고 그 '그룹' 안에서 자기들끼리 비교해요. '이 답안지는 우리 그룹 평균보다 좋네? 그럼 플러스!', '이건 평균보다 나쁘네? 마이너스!' 이런 식으로 상대평가를 하는 거죠. 덕분에 선생님 모델을 띄울 GPU 자원을 아껴서 더 많이 학습할 수 있게 된 겁니다."

3. 김수경 교수님의 'Good Liar' 현상
Q: 모델이 '믿지 않는 것'을 학습하면 왜 성능이 떨어지나요?

쉬운 설명: "사람도 자기가 전혀 동의하지 않는 거짓말을 계속 반복해서 믿으라고 강요받으면 사고 체계에 혼란이 오죠? 모델도 마찬가지입니다. 모델 내부에 구축된 **논리적인 연결망(신념)**이 있는데, 리워드를 받기 위해 억지로 그 연결망과 충돌하는 대답을 하도록 튜닝하면(Good Liar 상태), 모델 내부의 파라미터들이 꼬이기 시작합니다. 결국 이 '내적 갈등' 때문에 원래 잘 풀던 수학 문제나 논리 문제까지 엉뚱한 방식으로 풀게 되는 지능 저하가 발생하는 거예요. 그래서 교수님은 겉으로 보이는 성능 점수보다 모델이 정말 일관성 있게 대답하는지를 보는 게 중요하다고 강조하신 겁니다."

4. 양자화(Quantization)와 지능 저하
Q: Llama-3를 압축했는데 왜 유독 수학이나 코딩 능력이 떨어졌나요?

쉬운 설명: "양자화는 소수점 아래까지 정밀한 숫자를 딱딱 끊어지는 정수로 바꾸는 거예요. 일상적인 대화는 '오늘 날씨 좋아'나 '오늘 날씨 좋다'나 큰 차이가 없어서 괜찮습니다. 하지만 수학이나 코딩은 아주 미세한 숫자 차이나 기호 하나로 정답과 오답이 갈리죠. 과도한 압축을 하면 이런 **'미세한 논리적 차이'**를 나타내던 숫자 데이터들이 뭉개져 버립니다. 해상도가 너무 낮은 사진에서 작은 글씨를 읽을 수 없는 것과 똑같은 원리입니다."

5. 향후 개발 시 고려사항 (SLLM vs API)
Q: 자체 경량화 모델을 만드는 게 항상 유리할까요?

쉬운 설명: "비용과 보안의 문제입니다. GPT-4 같은 대형 모델의 API를 쓰는 건 **'최고급 레스토랑에서 밥을 사 먹는 것'**과 같아요. 편하고 맛있지만 비싸고, 우리가 무슨 메뉴를 먹었는지 레스토랑이 다 알게 되죠(보안 이슈). 반면 LoRA 같은 기법으로 만든 **경량 모델(SLLM)**은 **'내 집 주방에서 직접 요리하는 것'**입니다. 처음엔 도구도 사고 공부도 해야 하지만, 한 번 갖춰놓으면 싸고 보안도 완벽합니다. 특히 우리 회사만의 특수한 데이터(레시피)를 다뤄야 한다면 경량화 모델을 직접 구축하는 것이 장기적으로 훨씬 유리합니다."

💡 팁: 스터디원들이 더 깊게 파고들면 "DeepSeek-R1 논문을 보면 이 GRPO 방식이 수학 벤치마크에서 특히 효율적이었다고 나오니, 나중에 같이 논문의 Figure 2를 한번 뜯어볼까요?" 라고 제안해 보세요. 최고의 답변이 될 것입니다.

양자화(Quantization): FP32 → INT8은 무슨 의미인지도 위와 같이 답변해보자

스터디원들이 "FP32니 INT8이니 하는 용어가 정확히 뭘 뜻하느냐"고 묻는다면, 데이터의 '칸칸이(정밀도)' 개념으로 설명하는 것이 가장 명쾌합니다.

5. 양자화(Quantization): FP32 → INT8은 무슨 의미인가요?
용어 정의:

FP32 (32-bit Floating Point): '32비트 부동소수점'입니다. 소수점 아래 아주 먼 곳까지 정밀하게 표현하는 방식입니다.

INT8 (8-bit Integer): '8비트 정수'입니다. 소수점 없이 딱딱 끊어지는 숫자로 표현하는 방식입니다.

쉬운 설명: "모델이 가진 가중치(지식) 하나하나를 표현할 때, 원래는 **'아주 세밀한 눈금이 그려진 32비트짜리 긴 자'**를 사용했어요. 예를 들어 0.12345678... 처럼 아주 미세한 차이까지 다 기록한 거죠. 이게 바로 FP32입니다. 지능은 높지만 메모리를 엄청나게 차지합니다.

양자화는 이 긴 자를 버리고, **'눈금이 듬성듬성한 8비트짜리 짧은 자(INT8)'**로 바꾸는 과정입니다. 0.12345 같은 숫자를 그냥 '0'이나 '0.1' 근처의 정수값으로 반올림해서 퉁치는 거죠.

이렇게 하면 어떤 일이 벌어질까요?

메모리 절약: 숫자 하나당 차지하는 공간이 4분의 1로 확 줄어듭니다. (32비트 → 8비트)

속도 향상: GPU나 CPU는 복잡한 소수점 계산보다 단순한 정수 계산을 훨씬 빨리 처리합니다.

결국 FP32에서 INT8로 간다는 것은, '너무 정밀해서 무거운 백과사전'을 '핵심만 요약해서 가볍게 만든 요약본'으로 바꾸는 작업이라고 이해하시면 됩니다. 물론 너무 심하게 요약(INT4 등)하면 앞서 말씀드린 것처럼 지능이 떨어지는 부작용이 생기기도 합니다."

💡 스터디 발표를 위한 최종 팁
발표 중에 이 질문이 나오면 아래와 같이 덧붙여 보세요.

"실제로 우리가 쓰는 최신 GPU들은 이 INT8 연산을 아주 빠르게 처리하도록 설계되어 있어요. 그래서 양자화만 잘해도 모델의 지능은 거의 그대로인데, 속도는 몇 배나 빨라지는 마법을 경험할 수 있습니다."